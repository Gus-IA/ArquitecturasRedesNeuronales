# Deep Learning con MNIST, CIFAR-10 y Transformers

Este repositorio explora diferentes t√©cnicas de aprendizaje profundo aplicadas a visi√≥n computacional usando PyTorch y Scikit-learn. Abarca desde modelos b√°sicos como perceptrones hasta arquitecturas m√°s complejas como CNNs, LeNet-5, AlexNet, VGG, ResNet y Transformers.

---

## üìö Contenidos

El proyecto incluye:

### 1. **Clasificaci√≥n de MNIST**
- Descarga y preprocesamiento del dataset MNIST.
- Entrenamiento con un perceptr√≥n multicapa (MLP).
- Uso de redes convolucionales simples.

### 2. **An√°lisis y visualizaci√≥n de CIFAR-10**
- Descarga del dataset CIFAR-10.
- Conversi√≥n a escala de grises.
- Detecci√≥n de bordes usando convoluciones manuales.
- Visualizaci√≥n de filtros y pooling con PyTorch.

### 3. **Redes Neuronales Convolucionales (CNNs)**
- Implementaci√≥n de redes convolucionales personalizadas.
- Arquitecturas famosas: 
  - LeNet-5
  - AlexNet
  - VGG-16
  - ResNet-34

### 4. **Transformers para clasificaci√≥n y captioning**
- Divisi√≥n de im√°genes en parches (patches).
- Self-attention, multi-head attention y bloques de Transformer.
- Predicci√≥n de etiquetas en texto (captioning de d√≠gitos).
- Entrenamiento de un modelo Transformer que predice la palabra ("cero", "uno", etc.) correspondiente al d√≠gito en la imagen.

---

## ‚öôÔ∏è Instalaci√≥n

pip install -r requirements.txt

üßë‚Äçüíª Autor

Desarrollado por Gus como parte de su aprendizaje en Python e IA.
